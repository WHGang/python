# 什么是卷积？
对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。


# 1.CNN在图像上表现好的原因

平移不变性  
相邻空间特征  
局部连接；  
权值共享：减小参数量；  
池化操作：增大感受野；  
多层次结构：可以提取low-level以及high-level的信息；  

# 2、CNN不适用的场景？

1. 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。

2. 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。

# 2. Batch Normalization
1. 使学习快速进行（能够使用较大的学习率）
2. 降低模型对初始值的敏感性
3. 从一定程度上抑制过拟合
4. 防止梯度爆炸和梯度消失
先做了归一化，例如先减去期望再除以方差，然后再乘以一个参数和加上一个参数，这两个参数是可学习的

# 3.池化作用
1. 下采样，减小参数；
2. 平移不变性-Translation Invariance，其中不变形性包括，平移不变性、旋转不变性和尺度不变性。
   比如最常见的conv-max pooling，因为取一片区域的最大值，所以这个最大值在该区域内无论在哪，max-pooling之后都是它，相当于对微小位移的不变性。而如果我们对输入pooling前的层做一些精心设计，让pooling region里对应的是设计后的一些性质变化，那么pooling就相当于实现了对这种设计出来的变化的不变性。比如旋转(当然就旋转而言这个方法似乎不是很好)

3. 防止过拟合；
4. 可以扩大感知野；


# batch、mini-batch、随机梯度下降

1. 之前我们都是一次将所有图片输入到网络中学习，这种做法就叫batch梯度下降

2. 与batch对应的另一种极端方法是每次就只输入一张图片进行学习，我们叫随机梯度下降

3. 介于batch梯度下降和随机梯度下降之间的就是我们现在要整的，叫mini-batch梯度下降


# 1x1大小的卷积核的作用

1. 通过控制卷积核个数实现升维或者降维，从而减少模型参数
2. 对不同特征进行归一化操作
3. 用于不同channel上特征的融合


# 说一下nms的操作

对每一类进行nms，先根据score进行降序排序，然后计算最高的score和其他框的iou，去掉iou大于阈值的检测框

# 为什么要使用许多小卷积核(如3x 3 )而不是几个大卷积核?

## 1. [参考博客](https://blog.csdn.net/qq_40027052/article/details/79015827)
1.感受野  
2.计算量  
3.非线性  

## **2. 卷积输出公式**
out = (input + 2 * padding - kernel)/stride + 1     &:取不超过该值的最大整数

## **3. 参数量**
[参考](https://zhuanlan.zhihu.com/p/77471991)
1. 卷积核参数量：
计算公式：参数量=（kernel size * 前一层特征图的通道数 ）* 当前层kernel数量
2. 特征图参数量：
计算公式：参数量= output size 

3. 全连接层的参数量：
VGG-16最后一次卷积得到的feature map为 7 * 7 * 512，全连接层是将feature map展开成一维向量 1 * 4096 。实际上，我们就是用4096个 7 * 7 * 512的kernel去做卷积（可以理解为是一个卷积层）。
我们就可以计算第一个FC的参数量  7 * 7 * 512 * 4096 = 102762448。
1亿啊，这个数字好大的。这也是为什么说：全连接层参数冗余。全连接层参数就可占整个网络参数80%左右，好吓人的。


## **4. 计算量**

1. 卷积层
以VGG-16为例，Conv1-1，输入 224 * 224 * 3，64个 3 * 3 filter，输出feature map 224 * 224 * 64 。

feature map中的每一个像素点，都是64个 3*3 filter 共同作用于原图计算一次得到的，
所以它的计算量为 3 * 3 * 3 * 64。
已经知道单个像素的计算量，那乘以feature map所有像素，
就是一次卷积的计算量： 3 * 3 * 3 * 64 * 224 * 224。

2. 全连接层
VGG-16最后一次卷积得到的feature map为  7 * 7 * 512，全连接层是将feature map展开成一维向量 1 * 4096 。则FC层的计算量为 7 * 7 * 512 * 4096 = 102762448 。


# 什么是数据正则化/归一化(normalization)？为什么我们需要它？
我觉得这一点很重要。数据归一化是非常重要的预处理步骤，用于重新缩放输入的数值以适应特定的范围，从而确保在反向传播期间更好地收敛。一般来说采取的方法都是减去每个数据点的平均值并除以其标准偏差。如果我们不这样做，那么一些特征(那些具有高幅值的特征)将在cost函数中得到更大的加权(如果较高幅值的特征改变1 %，则该改变相当大，但是对于较小的特征，该改变相当小)。数据归一化使所有特征的权重相等。


# 卷积神经网络中空洞卷积的作用是什么？
答、空洞卷积也叫扩张卷积，在保持参数个数不变的情况下增大了卷积核的感受野，同时它可以保证输出的特征映射（feature map）的大小保持不变。一个扩张率为2的3×3卷积核，感受野与5×5的卷积核相同，但参数数量仅为9个


# 如何解决数据不平衡问题？
1. 利用重采样中的下采样和上采样，对小数据类别采用上采用，通过复制来增加数据，不过这种情况容易出现过拟合，建议用数据扩增的方法，对原有数据集进行翻转，旋转，平移，尺度拉伸，对比度，亮度，色彩变化来增加数据。对大数据类别剔除一些样本量。

2. 组合不同的重采样数据集：假设建立十个模型，选取小数据类1000个数据样本，然后将大数据类别10000个数据样本分为十份，每份为1000个，并训练十个不同的模型。

3. 更改分类器评价指标： 在传统的分类方法中，准确率是常用的指标。 然而在不平衡数据分类中，准确率不再是恰当的指标，采用精准率即查准率P：真正例除以真正例与假正例之和。召回率即查全率F。真正例除以真正例与假反例之和。或者F1分数查全率和查准率加权平衡=2*P*R/(P+R)。


# 常用的激活函数和优缺点？
@ https://www.it610.com/article/1279354065553211392.htm


# 评价指标有哪些？

[参考](https://blog.csdn.net/weixin_44791964/article/details/104695264)
准确率：在所有预测结果是正类中，实际也是正类的概率。
召回率：在所有实际是正类结果中，预测也是正类结果的概率。
MAP:


# 当神经网络效果不好时，应该从哪些方面考虑解决该问题？
1. 是否找到合适的损失函数？（不同问题适合不同的损失函数）（理解不同损失函数的适用场景）

2. batch size是否合适？batch size太大 -> loss很快平稳，batch size太小 -> loss会震荡（理解mini-batch）

3. 是否选择了合适的激活函数？（各个激活函数的来源和差异）

4. 学习率，学习率小收敛慢，学习率大loss震荡（怎么选取合适的学习率）

5. 是否选择了合适的优化算法？（比如adam）（理解不同优化算法的适用场景）

6. 是否过拟合？(深度学习拟合能力强，容易过拟合)（理解过拟合的各个解决方案）

a. Early Stopping
b. Regularization（正则化）
c. Weight Decay（收缩权重）
d. Dropout（随机失活）
e. 调整网络结构


# K-means算法流程？它与KNN的区别？
[参考1](https://blog.csdn.net/chlele0105/article/details/12997391)
[参考2](https://zhuanlan.zhihu.com/p/31580379?utm_source=qq)

（1）K-means的基本算法流程：

a. 初始化k个聚类中心c1,c2,...,ck；

b. 对于每个样本xi和 每个聚类中心cj，计算样本与聚类中心之间的距离dij；

c. 对于每个样本xi，基于其最小的dij把其分配到第j个类Cj；

d. 对于每个类Cj，计算其所有样本均值作为新的聚类中心，重复步骤2和步骤3直至样本点所属的类不再变化或达到最大迭代次数；

（2）K-means与KNN的区别：

a. K-means是无监督学习算法，KNN是有监督学习算法;

b. K-means有明显的训练过程（求聚类中心），KNN在学习阶段只是简单的把所有样本记录;

c. 在测试阶段，对于K-means，新的样本点的判别与聚类中心有关，即与所有训练样本有关，对于KNN，新的样本点的判别只是与最近邻的K个样本有关;



# [深度学习-优化]欠拟合与过拟合以及解决方法
@https://blog.csdn.net/keeppractice/article/details/107026566  
@https://mofanpy.com/tutorials/machine-learning/torch/intro-overfitting/







# Faster-rcnn
## Roi-Pooling 
[参考](https://zhuanlan.zhihu.com/p/73662410)


# SSD



# Yolo






